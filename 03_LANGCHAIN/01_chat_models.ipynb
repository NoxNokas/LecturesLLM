{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c781fc24",
   "metadata": {},
   "source": [
    "# ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3102c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самолет - это летательный аппарат, способный взлетать и приземляться.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0.1)\n",
    "\n",
    "user_message = \"Что такое самолёт?\"\n",
    "\n",
    "answer = llm.invoke(user_message)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0c96b",
   "metadata": {},
   "source": [
    "Метод invoke возвращает не просто текст ответа, а объект класса AIMessage, содержащий довольно много различной информации, которую мы уже могли наблюдать, когда обращались к Ollama через API. Фактически под капотом метода invoke выполняется вызов указанной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e15319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Самолет - это летательный аппарат, способный взлетать и приземляться.' additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-08-18T14:53:15.03922012Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3699309021, 'load_duration': 1039332128, 'prompt_eval_count': 16, 'prompt_eval_duration': 563668267, 'eval_count': 21, 'eval_duration': 2094460360, 'model_name': 'llama3.1:8b'} id='run--0a34c786-0fb4-4ba0-9e62-61d39c6461d7-0' usage_metadata={'input_tokens': 16, 'output_tokens': 21, 'total_tokens': 37}\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d6a0c",
   "metadata": {},
   "source": [
    "# Messages\n",
    "\n",
    "Во фреймворке LangChain сообщения выделены в отдельные классы: AIMessage, HumanMessage и SystemMessage, соответствующие различным ролям и наследующиеся от класса BaseMessage. При вызове модели мы можем передать список сообщений, чтобы модель могла учитывать его при ответе. Например, в коде ниже модель переводит сообщения пользователя с русского на английский язык."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d552abc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I will create a successful AI product!' additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-08-18T14:53:17.295884357Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2193697107, 'load_duration': 28463509, 'prompt_eval_count': 40, 'prompt_eval_duration': 1309666152, 'eval_count': 9, 'eval_duration': 855107802, 'model_name': 'llama3.1:8b'} id='run--19d7b382-e2a3-490b-aa5f-2a0f4048d9f3-0' usage_metadata={'input_tokens': 40, 'output_tokens': 9, 'total_tokens': 49}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You translate Russian to English. Translate the user sentence and write only result:\"),\n",
    "    HumanMessage(content=\"Я создам успешный AI-продукт!\")\n",
    "]\n",
    "\n",
    "answer = llm.invoke(messages)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286145fe",
   "metadata": {},
   "source": [
    "# Дополнительные методы stream, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15244ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_1 = [\n",
    "    SystemMessage(content=\"You translate Russian to English. Translate the user sentence and write only result:\"),\n",
    "    HumanMessage(content=\"Я создам успешный AI-продукт!\")\n",
    "]\n",
    "messages_2 = [\n",
    "    SystemMessage(content=\"You translate Russian to English. Translate the user sentence and write only result:\"),\n",
    "    HumanMessage(content=\"У меня ничего не получится!\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfaacc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will create a successful AI product!"
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "# ChatModel поддерживает стриминговый или потоковый тип исполнения, при котором ответ модели возвращается по мере его генерации.\n",
    "# В качестве ответа возвращается объект класса AIMessageChunk. Попробуем заменить вывод модели из прошлого примера следующими двумя строчками.\n",
    "# Это может быть полезно, когда мы не хотим дожидаться конца ответа модели, а хотим видеть результат сразу же.\n",
    "for message_chunk in llm.stream(messages_1):\n",
    "    print(message_chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5118a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will create a successful AI product!\n",
      "Nothing is working out for me!\n"
     ]
    }
   ],
   "source": [
    "# Batch или пакетный, при котором модель может почти одновременно обрабатывать несколько запросов, что повышает эффективность обработки данных.\n",
    "ai_message_1, ai_message_2 = llm.batch([messages_1, messages_2])\n",
    "print(ai_message_1.content)\n",
    "print(ai_message_2.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
